{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "\"\"\"\n",
    "Bounding‑box pipeline **v5 – deux colonnes dédiées**\n",
    "===================================================\n",
    "\n",
    "Changements demandés :\n",
    "1. **Suppression totale** de la colonne `grasp_points`.\n",
    "2. Deux nouvelles colonnes fixes :\n",
    "   * `box.pickup`   – les 4 floats du cercle orange (objet à saisir).\n",
    "   * `box.target`   – les 4 floats de la boîte noire (zone cible).\n",
    "\n",
    "Format fidèle à gr00t : chaque colonne est une *liste plate de longueur\n",
    "4* (`[ymin,xmin,ymax,xmax]`, valeurs normalisées ∈ [0‑1]).  Gr00t peut\n",
    "encore faire `np.asarray(col, dtype=np.float32)` sans surprise.\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────────────── Config & model ────────────────────────────\n",
    "MODEL_ID = \"google/paligemma-3b-mix-224\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "DTYPE = torch.float16 if DEVICE.type in {\"cuda\", \"mps\"} else torch.float32\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = (\n",
    "    PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=DTYPE)\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "ZERO_BOX = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "VIDEO_DIR   = os.path.expanduser(\n",
    "    \"/home/pa-boss/Isaac-GR00T/bounding-box-test1/videos/chunk-000/observation.images.secondary_0\"\n",
    ")\n",
    "PARQUET_DIR = os.path.expanduser(\n",
    "    \"/home/pa-boss/Isaac-GR00T/bounding-box-test1/data/chunk-000\"\n",
    ")\n",
    "OUT_DIR = \"/home/pa-boss/Isaac-GR00T/notebooks/outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ───────────────────── Helper : normalisation [0‑1] ─────────────────────\n",
    "\n",
    "def _norm(v, dim):\n",
    "    if 0.0 <= v <= 1.0:\n",
    "        return float(v)\n",
    "    if 0.0 <= v <= 1000.0:\n",
    "        return float(v) / 1000.0\n",
    "    return float(v) / dim\n",
    "\n",
    "\n",
    "def _norm_box(box, h, w):\n",
    "    y1,x1,y2,x2 = box\n",
    "    return [_norm(y1,h), _norm(x1,w), _norm(y2,h), _norm(x2,w)]\n",
    "\n",
    "# ─────────────── Parsing PaliGemma (no label info) ────────────────\n",
    "_loc_re = re.compile(r\"<loc(\\d+)>\")\n",
    "\n",
    "def _parse_pg(txt):\n",
    "    boxes = []\n",
    "    for chunk in re.split(r\"[;\\n]\", txt):\n",
    "        locs = _loc_re.findall(chunk)\n",
    "        if len(locs) == 4:\n",
    "            boxes.append(list(map(float, locs)))\n",
    "    return boxes  # order assumed: orange then black (prompt order)\n",
    "\n",
    "# ─────────────── Classical fallback w/ explicit labels ───────────────\n",
    "_warned = False\n",
    "\n",
    "def _detect_orange(img):\n",
    "    hsv  = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv,(5,100,100),(25,255,255))\n",
    "    mask = cv2.GaussianBlur(mask,(9,9),2)\n",
    "    circ = cv2.HoughCircles(mask,cv2.HOUGH_GRADIENT,1,50,param1=100,param2=15,minRadius=10,maxRadius=200)\n",
    "    if circ is None: return None\n",
    "    x,y,r = circ[0][0]\n",
    "    return [y-r,x-r,y+r,x+r]\n",
    "\n",
    "def _detect_black(img):\n",
    "    g = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    _,th = cv2.threshold(g,50,255,cv2.THRESH_BINARY_INV)\n",
    "    cnts,_ = cv2.findContours(th,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts: return None\n",
    "    x,y,w,h = cv2.boundingRect(max(cnts,key=cv2.contourArea))\n",
    "    return [y,x,y+h,x+w]\n",
    "\n",
    "def _fallback(img, raw):\n",
    "    global _warned\n",
    "    if not _warned:\n",
    "        print(\"⚠️  Fallback – raw PG sample:\\n\", raw[:200], \"…\")\n",
    "        _warned = True\n",
    "    return [_detect_orange(img), _detect_black(img)]\n",
    "\n",
    "# ───────────────────── Detection wrapper (returns 2) ─────────────────────\n",
    "\n",
    "def detect_two_boxes(frame_bgr):\n",
    "    \"\"\"Return (box.pickup, target_box) each length‑4 list normalised 0‑1.\"\"\"\n",
    "    pil = Image.fromarray(cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)).resize((224,224))\n",
    "    print(f\"Shape of pil: {pil}\")\n",
    "    inp = processor(text=\"<Image> detect orange circle ; black box\", images=pil, return_tensors=\"pt\")\n",
    "    inp = {k:v.to(DEVICE) for k,v in inp.items()}\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inp, max_length=inp[\"input_ids\"].shape[-1]+100)\n",
    "    raw = processor.batch_decode(ids[:, inp[\"input_ids\"].shape[-1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    boxes = _parse_pg(raw)\n",
    "    if len(boxes) < 2:\n",
    "        boxes = _fallback(frame_bgr, raw)\n",
    "    pickup  = boxes[0] if len(boxes) >= 1 and boxes[0] is not None else ZERO_BOX\n",
    "    target  = boxes[1] if len(boxes) >= 2 and boxes[1] is not None else ZERO_BOX\n",
    "\n",
    "    h,w = frame_bgr.shape[:2]\n",
    "    return _norm_box(pickup,h,w), _norm_box(target,h,w)\n",
    "\n",
    "# ───────────────────── DataFrame helpers ─────────────────────\n",
    "\n",
    "def _ensure_vec(df, col):\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "    for idx in df.index:\n",
    "        v = df.at[idx,col]\n",
    "        if not (isinstance(v,list) and len(v)==4 and all(isinstance(x,(int,float)) for x in v)):\n",
    "            df.at[idx,col] = ZERO_BOX\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75257424C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:   5%|▌         | 1/20 [00:00<00:06,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000000] pickup=[0.311, 0.373, 0.552, 0.509] target=[0.649, 0.324, 0.715, 0.351]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7526B49A5720>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  10%|█         | 2/20 [00:00<00:06,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000001] pickup=[0.33, 0.358, 0.559, 0.49] target=[0.469, 0.29, 0.539, 0.32]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7526B49A5720>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  15%|█▌        | 3/20 [00:01<00:05,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000002] pickup=[0.359, 0.338, 0.591, 0.46] target=[0.692, 0.346, 0.759, 0.379]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7526B49A5720>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  20%|██        | 4/20 [00:01<00:05,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000003] pickup=[0.363, 0.33, 0.583, 0.463] target=[0.376, 0.523, 0.455, 0.557]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7526B49A5720>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  25%|██▌       | 5/20 [00:01<00:05,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000004] pickup=[0.361, 0.334, 0.586, 0.455] target=[0.282, 0.52, 0.359, 0.557]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7526B49A5720>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  30%|███       | 6/20 [00:02<00:05,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000005] pickup=[0.322, 0.498, 0.4, 0.534] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7525794B87F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  35%|███▌      | 7/20 [00:02<00:04,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000006] pickup=[0.568, 0.523, 0.634, 0.557] target=[0.907, 0.19, 4.2625, 0.35]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75256D2A4460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  40%|████      | 8/20 [00:03<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000007] pickup=[0.269, 0.463, 0.347, 0.503] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x752574112860>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  45%|████▌     | 9/20 [00:03<00:04,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000008] pickup=[0.276, 0.203, 0.475, 0.27] target=[0.45, 0.121, 0.52, 0.158]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x752574112860>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  50%|█████     | 10/20 [00:03<00:03,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000009] pickup=[0.28, 0.208, 0.451, 0.286] target=[0.392, 0.121, 0.455, 0.158]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75256D2A5210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  55%|█████▌    | 11/20 [00:04<00:03,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000010] pickup=[0.308, 0.261, 0.448, 0.344] target=[0.321, 0.485, 0.392, 0.52]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7525794B87F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  60%|██████    | 12/20 [00:04<00:02,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000011] pickup=[0.219, 0.39, 0.29, 0.422] target=[0.267, 0.208, 0.449, 0.258]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x752574112860>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  65%|██████▌   | 13/20 [00:04<00:02,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000012] pickup=[0.267, 0.344, 0.326, 0.377] target=[0.282, 0.215, 0.462, 0.286]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x7525794B87F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  70%|███████   | 14/20 [00:05<00:02,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000013] pickup=[0.338, 0.136, 0.411, 0.167] target=[0.0, 0.0, 4.2625, 3.1625]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75256D2B1E40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  75%|███████▌  | 15/20 [00:05<00:02,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000014] pickup=[0.359, 0.118, 0.43, 0.155] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75257424C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  80%|████████  | 16/20 [00:07<00:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000015] pickup=[0.269, 0.152, 0.354, 0.182] target=[0.0, 0.0, 4.2625, 3.1625]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75257424C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  85%|████████▌ | 17/20 [00:08<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000016] pickup=[0.241, 0.242, 0.416, 0.305] target=[0.402, 0.579, 0.469, 0.612]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75257424C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  90%|█████████ | 18/20 [00:10<00:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000017] pickup=[0.288, 0.41, 0.363, 0.44] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x75257424C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes:  95%|█████████▌| 19/20 [00:10<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000018] pickup=[0.351, 0.107, 0.415, 0.138] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "Shape of pil: <PIL.Image.Image image mode=RGB size=224x224 at 0x752574112860>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episodes: 100%|██████████| 20/20 [00:11<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode_000019] pickup=[0.291, 0.182, 0.367, 0.221] target=[0.0, 0.0, 4.2625, 3.196875]\n",
      "✅ All episodes updated with pickup_box / target_box (grasp_points supprimé).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── Main loop ─────────────────────\n",
    "clips = sorted(f for f in os.listdir(VIDEO_DIR) if f.startswith(\"episode_\") and f.endswith(\".mp4\"))\n",
    "\n",
    "for vid in tqdm(clips, desc=\"episodes\"):\n",
    "    cap = cv2.VideoCapture(os.path.join(VIDEO_DIR, vid))\n",
    "    ok, frame = cap.read(); cap.release()\n",
    "    if not ok:\n",
    "        print(\"❌\", vid, \"frame0 fail\"); continue\n",
    "\n",
    "    pickup_box, target_box = detect_two_boxes(frame)\n",
    "    \n",
    "\n",
    "    # Write to parquet ---------------------------------------------------\n",
    "    episode = vid[:-4]\n",
    "    pq_path = os.path.join(PARQUET_DIR, f\"{episode}.parquet\")\n",
    "    if not os.path.exists(pq_path):\n",
    "        print(\"❌ parquet missing for\", vid); continue\n",
    "\n",
    "    df = pd.read_parquet(pq_path)\n",
    "    df = _ensure_vec(df, \"box.pickup\")\n",
    "    df = _ensure_vec(df, \"box.target\")\n",
    "\n",
    "    first = df.index[0]\n",
    "    df.at[first, \"box.pickup\"] = pickup_box\n",
    "    df.at[first, \"box.target\"] = target_box\n",
    "\n",
    "    # Option : répliquer sur toutes les lignes (stats > robustes)\n",
    "    df.loc[:, \"box.pickup\"] = df.loc[:, \"box.pickup\"].apply(lambda _: pickup_box)\n",
    "    df.loc[:, \"box.target\"] = df.loc[:, \"box.target\"].apply(lambda _: target_box)\n",
    "\n",
    "    df.to_parquet(pq_path)\n",
    "\n",
    "    # ─── debug image → OUT_DIR ----------------------------------------\n",
    "    dbg = frame.copy()\n",
    "\n",
    "    # rectangle rouge (pickup)\n",
    "    x1 = int(pickup_box[1] * frame.shape[1])  # xmin * width\n",
    "    y1 = int(pickup_box[0] * frame.shape[0])  # ymin * height\n",
    "    x2 = int(pickup_box[3] * frame.shape[1])  # xmax * width\n",
    "    y2 = int(pickup_box[2] * frame.shape[0])  # ymax * height\n",
    "    cv2.rectangle(dbg, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "    # rectangle bleu (target)\n",
    "    x1 = int(target_box[1] * frame.shape[1])\n",
    "    y1 = int(target_box[0] * frame.shape[0])\n",
    "    x2 = int(target_box[3] * frame.shape[1])\n",
    "    y2 = int(target_box[2] * frame.shape[0])\n",
    "    cv2.rectangle(dbg, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUT_DIR, f\"{vid[:-4]}_dbg.png\"), dbg)\n",
    "\n",
    "    print(f\"[{episode}] pickup={pickup_box} target={target_box}\")\n",
    "\n",
    "print(\"✅ All episodes updated with pickup_box / target_box (grasp_points supprimé).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
